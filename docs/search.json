[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "MSA Background",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "MSA Background"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot.",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nAnalysis Timeframe To reiterate, we are looking at identify trends we will need to look at data for a period that begins before the amendments came into effect and ends with the most recent data available. However, we are constrained by the data available through NOAA’s StockSMART database - the most publically-accessible comprehensive repository on fisheries resource management. As such, we are looking at the time period between 2005 and 2022.\nAnalysis Plan\nDatasets Used",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atlantic Coast Working Waterfronts from Delaware to Maine",
    "section": "",
    "text": "Commercial fishing is a key industry in the Northeast. Historically, fisheries were the primary employer in many towns, and cities grew up around the industry. Throughout the 20th and 21st centuries, commercial fishing had periods of boom and bust leading to declining towns and depopulation along the coast. Despite these fluctuations, commercial fishing is still important to the region both economically and culturally. The industry also has significant environmental externalities, including pollution, trophic system collapse and ocean warming. Managing fishery operations is essential to ensure long term resource availability but can have negative impacts in the short term on the economic viability of businesses. This is a question of balancing economy and livelihoods with sustainability.\nTo this end, the Magnuson-Stevens Fishery Conservation and Management Act (MSA) is the primary law responsible for commercial fishery management in the United States. The federal government passed the act in 1976, and reauthorized it in 2007 and 2018. At the time of both re-authorizations, the federal government incorporated additional regulations. We are keen to understand what impacts the 2007 and 2018 amendments to the MSA have had on coastal communities in the Northeast and if the amendments were successful in conserving Atlantic fish stocks. To read more about the act, please click on the MSA Background tab on the left.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550: Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Atlantic Coast Working Waterfronts from Delaware to Maine",
    "section": "",
    "text": "Commercial fishing is a key industry in the Northeast. Historically, fisheries were the primary employer in many towns, and cities grew up around the industry. Throughout the 20th and 21st centuries, commercial fishing had periods of boom and bust leading to declining towns and depopulation along the coast. Despite these fluctuations, commercial fishing is still important to the region both economically and culturally. The industry also has significant environmental externalities, including pollution, trophic system collapse and ocean warming. Managing fishery operations is essential to ensure long term resource availability but can have negative impacts in the short term on the economic viability of businesses. This is a question of balancing economy and livelihoods with sustainability.\nTo this end, the Magnuson-Stevens Fishery Conservation and Management Act (MSA) is the primary law responsible for commercial fishery management in the United States. The federal government passed the act in 1976, and reauthorized it in 2007 and 2018. At the time of both re-authorizations, the federal government incorporated additional regulations. We are keen to understand what impacts the 2007 and 2018 amendments to the MSA have had on coastal communities in the Northeast and if the amendments were successful in conserving Atlantic fish stocks. To read more about the act, please click on the MSA Background tab on the left.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "MSABackground.html",
    "href": "MSABackground.html",
    "title": "MSA Background",
    "section": "",
    "text": "The Magnuson-Stevens Fishery Conservation and Management Act (MSA) is the primary law responsible for commercial fishery management in the United States. First passed in 1976, the MSA aims to:\n\n\nprevent overfishing,\nrebuild overfished stocks,\nensure a safe and sustainable seafood supply,\nincrease long-term economic and social benefit, and\nprotect fish habitats needed for spawning, breeding, feeding, and growing to maturity.\n\n\nThe federal government reauthorized and strengthened the MSA in 2007. In 2018, the MSA was further amended by the Modernizing Recreational Fisheries Management Act to improve the accountability of mixed-use fisheries (companies that have commercial and recreational operations). We are keen to understand what impacts the 2007 and 2018 amendments to the MSA have had on coastal communities in the Northeast.",
    "crumbs": [
      "MSA Background"
    ]
  },
  {
    "objectID": "analysis/Fisheries_NE.html",
    "href": "analysis/Fisheries_NE.html",
    "title": "FINAL PROJECT",
    "section": "",
    "text": "---\nformat: \n  html:\n    toc: false\n    page-layout: full\nexecute:\n    echo: false\n---\nMariya Lupandina and Clarasophia Gust MUSA 550 | Fall 2024\n# Import necessary libraries \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport zipfile"
  },
  {
    "objectID": "analysis/Fisheries_NE.html#question-1",
    "href": "analysis/Fisheries_NE.html#question-1",
    "title": "FINAL PROJECT",
    "section": "QUESTION 1:",
    "text": "QUESTION 1:\nHow was fish stock, GDP, and commercial fishing employment impacted by the two policy changes?\n\nQ1 PART 1 / Regional Scale: Is there a regional pattern?\n\nFish Stock:\nData on fish stock counts from 2005 to 2022 was downloaded from NOAA Stock SMART (https://apps-st.fisheries.noaa.gov/stocksmart?app=download-data). Data on all stocks was downloaded for the two relevant marine ecosystem areas: Atlantic Highly Migratory (AHM) and Northeast Shelf (NES).\n\n# Import fish count data for 2005-2022 from NOAA StockSMART. \n# Data is downloaded by ecosystem area. Load xlsx data for Atlantic Highly Migratory (AHM) and Northeast Shelf (NES) ecosystems. \n    \nAHM_stocks = pd.read_csv(\"./data/AHM_NOAAstockSMART_2005to2022.csv\")\nNES_stocks = pd.read_csv(\"./data/NES_NOAAstockSMART_2005to2022.csv\")\n   \n# Drop rows without values for the ratio of current estimated total biomass and the estimate of total sustainable level of biomass (B/Bmsy).\nAHMstocks_clean = AHM_stocks.dropna(subset=['B/Bmsy'])\nNESstocks_clean = NES_stocks.dropna(subset=['B/Bmsy'])\n\n# Group each ecosystem's data by stock assessment year and species. \nAHM_grouped = AHMstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nNES_grouped = NESstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nstocks_df = combined_df = pd.concat([AHM_grouped, NES_grouped], ignore_index=True)\n\nNES_grouped.columns\n\nAtlantic Highly Migratory data:\n\nAHM_group = AHMstocks_clean.groupby(['Common Name', 'Assessment Year']).median('B/Bmsy').reset_index()\n\n# Ensure the 'Assessment Year' is treated as an integer\nAHMstocks_clean.loc[:, 'Assessment Year'] = AHMstocks_clean['Assessment Year'].astype(int)\n\nB_Bmsy_range = [0, 2.8]  # Example range, adjust this as needed\n\n\n# Create the plot\nAHMchart = alt.Chart(AHM_group).mark_line().encode(\n    x='Assessment Year:O', \n    y='B/Bmsy:Q',\n    color='Common Name:N',\n    detail='Common Name:N'\n).properties(\n    width=400,\n    height=400,\n    title=\"B/Bmsy for Each Species Over the Years 2005-2022\"\n)\n\n# Chart the 2007 and 2018 policies\npolicy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n)\n\n# Combine the line chart and vertical lines\nAHM_policychart = AHMchart + policy_lines\n\n# Display the final chart\nAHM_policychart\n\n\n\n\n\n\n\n\nNortheast Shelf Data: *** STILL NEED TO COMPLETE\n\n\nLandings // Fish Haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n\n\nRegional GDP\n\n# Import annual GDP data for each state of interest. \n\n\n# Filter data for the years 2005-2022. \n\n\n# Find mean GDP across all states in our study region for each year from 2005-2022. \n\n\n\n\nEmployment in Fishing Industry\n\n# Import data from Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for all states of interest for 2005-2022\n\n\n# Group data by state and year\n\n\n\n\nCombine all Data in One Graph\n\n# Create a graph combining all three data groups. \n\n# Chart the 2007 and 2018 policies\n# policy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    # x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n# )\n\n# Combine the line chart and vertical lines\n# finalchart = finalchart + policy_lines\n\n# Display the final chart\n# finalchart\n\n\n\n\nQ1 PART 2 / State Scale: Are there state-specific patterns?\nDownload states and county geometries\n\nStates_of_interest = ['Delaware', 'Pennsylvania', 'New Jersey', 'New York', 'Connecticut', 'Rhode Island', 'Massachusetts', 'New Hampshire', 'Maine']\n\n# Open downloaded US Census states shapefile from data folder\nallstates = gpd.read_file(\"./data/cb_2021_us_state_20m/cb_2021_us_state_20m.shp\")\nsel_states = allstates[allstates['NAME'].isin(States_of_interest)]\n\n# Download US coastal counties \n\n\nGDP per Capita\nDetermine GDP per capita for each state from 2005 to 2022\n\n# Import state annual GDP for 2005-2022 by county \n\n\n# Import US Census population count for each state from 2005-2022. \n\n\n# Calculate GDP per capita for each state for 2005-2022. \n\n\n# Calculate the median GDP per capita for 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nEmployment in Fishing Industry\nBureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for each state of interest from 2005 2022\n\n# Import Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county\n\n\n# Group data by yaer and county\n\n\n# Calculate median employment in fishing industry in each state from 2005-2007, 2008-2018, and 2019-2022.\n\n\n\nLandings // Fish haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n# Take median of landings in each state from 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nSpatial Join\n\n# Join GDP per capita, BLS employment, and landings data to the coastal counties gpd. \n\n\n\n\n\nInteractive Map\n\n# Create interactive map of coastal counties from Delaware to Maine region which toggles the three data groups"
  },
  {
    "objectID": "analysis/Fisheries_NE.html#question-2",
    "href": "analysis/Fisheries_NE.html#question-2",
    "title": "FINAL PROJECT",
    "section": "QUESTION 2:",
    "text": "QUESTION 2:\n\nWhere are the most important fisheries along the north Atlantic Coast and what factors define the most important fishing ports?\nSelect based on: - 2022 data - % of regional fish quantity - % GDP from maritime activities of county - % of employment in fishing - Density of fishing vessel activity\n\n\nDownload required libraries\n\nfrom urllib.request import urlopen\nimport json\nimport datetime\nimport requests\n\n\n\nSpecify Region of Interest\nFor this project, we are interested in the fisheres along the north Atlantic Coast from Delaware to Maine.\n\n# Create list of States of Interest\n\nStates_short = ['DE', 'PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'ME']\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n\n\nLandings in 2022 (quantity of fish)\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": \"2022\"}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\nlen(landings_df)\n\n10000\n\n\n\n\nFishing Vessel Activity\n\n# Request vessel data from NOAA Fisheries API \n###(NOTE TO MARIYA: I'm not sure whether it is pulling all of the pages of data. I tried to write a code which cycled through the data pages but couldn't get it to work.)\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/vessel_data/?offset=0&limit=50000\"\nparamsvessel = {\"hail_state\": States_short}\nresponse = requests.get(url, params=paramsvessel)\nvesseldata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = vesseldata['items']\nvessels_df = pd.DataFrame(items)\nlen(vessels_df)\n\n10000"
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\nText goes here\nBelow are things to aid formatting:\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "Conclusion"
    ]
  },
  {
    "objectID": "index.html#proposal-hypothesis",
    "href": "index.html#proposal-hypothesis",
    "title": "Atlantic Coast Working Waterfronts from Delaware to Maine",
    "section": "Proposal & Hypothesis",
    "text": "Proposal & Hypothesis\nWith out analysis we hope to answer questions in two buckets:\n\n\nHow were fish stock, GDP, and commercial fishing employment impacted by the two policy changes? Is there a regional pattern? Are there state-specific patterns?\nWhere are the most important fisheries along the north Atlantic Coast and what factors define the most important fishing ports?\n\nTo understand the We will be looking at the time period between 2005 and 2022 to include data before and after the amendments. We are also setting this time period based on the data available from NOAA’s StockSMART database, this is our constraining factor.\nWe are also using the project to supplement our work at the EMLab, which focuses on the coastal region from Delaware to Maine. Through the project we hope to learn more about a key socio-cultural facet of the Northeast coast that may inform our approach to designing nature-based infrastructures that brings environmental, economic and social benefits.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#proposal",
    "href": "index.html#proposal",
    "title": "Atlantic Coast Working Waterfronts from Delaware to Maine",
    "section": "Proposal",
    "text": "Proposal\nWith our analysis we hope to answer questions in two buckets:\n\n\nHow were fish stock, GDP, and commercial fishing employment impacted by the two policy changes? Is there a regional pattern? Are there state-specific patterns?\nWhere are the most important fisheries along the north Atlantic Coast and what factors define the most important fishing ports?\n\nTo identify trends we will need to look at data for a period that begins before the amendments came into effect and ends with the most recent data available. However, we are constrained by the data available through NOAA’s StockSMART database - the most publically-accessible comprehensive repository on fisheries resource management. As such, we are looking at the time period between 2005 and 2022.\nSHOULD WE LINK TO EMLAB WORK? OR NO BC WE DIDN’T ASK FOR PERMISSION FROM SEAN AND KEITH?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "analysis/2-MajorFisheriesNE.html",
    "href": "analysis/2-MajorFisheriesNE.html",
    "title": "Major Fisheries along the North Atlantic Coast",
    "section": "",
    "text": "---\nformat: \n  html:\n    toc: false\n    page-layout: full\nexecute:\n    echo: false\n---\n\nQUESTION 2: Where are the most important fisheries along the north Atlantic Coast and what factors define the most important fishing ports?\nSelect based on: - 2022 data - % of regional fish quantity - % GDP from maritime activities of county - % of employment in fishing - Density of fishing vessel activity\nMariya Lupandina and Clarasophia Gust MUSA 550 | Fall 2024\n\nDownload required libraries\n\n# Import necessary libraries \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport zipfile\nfrom urllib.request import urlopen\nimport json\nimport datetime\nimport requests\n\n\n\nSpecify Region of Interest\nFor this project, we are interested in the fisheres along the north Atlantic Coast from Delaware to Maine.\n\n# Create list of States of Interest\n\nStates_short = ['DE', 'PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'ME']\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n\n\nLandings in 2022 (quantity of fish)\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": \"2022\"}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\nlen(landings_df)\n\n10000\n\n\n\n\nFishing Vessel Activity\n\n# Request vessel data from NOAA Fisheries API \n###(NOTE TO MARIYA: I'm not sure whether it is pulling all of the pages of data. I tried to write a code which cycled through the data pages but couldn't get it to work.)\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/vessel_data/?offset=0&limit=50000\"\nparamsvessel = {\"hail_state\": States_short}\nresponse = requests.get(url, params=paramsvessel)\nvesseldata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = vesseldata['items']\nvessels_df = pd.DataFrame(items)\nlen(vessels_df)\n\n10000",
    "crumbs": [
      "Analysis",
      "Major Fisheries along the North Atlantic Coast"
    ]
  },
  {
    "objectID": "analysis/2-MajorFisheriesNE.html#question-1",
    "href": "analysis/2-MajorFisheriesNE.html#question-1",
    "title": "Major Fisheries along the North Atlantic Coast",
    "section": "QUESTION 1:",
    "text": "QUESTION 1:\nHow was fish stock, GDP, and commercial fishing employment impacted by the two policy changes?\n\nQ1 PART 1 / Regional Scale: Is there a regional pattern?\n\nFish Stock:\nData on fish stock counts from 2005 to 2022 was downloaded from NOAA Stock SMART (https://apps-st.fisheries.noaa.gov/stocksmart?app=download-data). Data on all stocks was downloaded for the two relevant marine ecosystem areas: Atlantic Highly Migratory (AHM) and Northeast Shelf (NES).\n\n# Import fish count data for 2005-2022 from NOAA StockSMART. \n# Data is downloaded by ecosystem area. Load xlsx data for Atlantic Highly Migratory (AHM) and Northeast Shelf (NES) ecosystems. \n    \nAHM_stocks = pd.read_csv(\"./data/AHM_NOAAstockSMART_2005to2022.csv\")\nNES_stocks = pd.read_csv(\"./data/NES_NOAAstockSMART_2005to2022.csv\")\n   \n# Drop rows without values for the ratio of current estimated total biomass and the estimate of total sustainable level of biomass (B/Bmsy).\nAHMstocks_clean = AHM_stocks.dropna(subset=['B/Bmsy'])\nNESstocks_clean = NES_stocks.dropna(subset=['B/Bmsy'])\n\n# Group each ecosystem's data by stock assessment year and species. \nAHM_grouped = AHMstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nNES_grouped = NESstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nstocks_df = combined_df = pd.concat([AHM_grouped, NES_grouped], ignore_index=True)\n\nNES_grouped.columns\n\nAtlantic Highly Migratory data:\n\nAHM_group = AHMstocks_clean.groupby(['Common Name', 'Assessment Year']).median('B/Bmsy').reset_index()\n\n# Ensure the 'Assessment Year' is treated as an integer\nAHMstocks_clean.loc[:, 'Assessment Year'] = AHMstocks_clean['Assessment Year'].astype(int)\n\nB_Bmsy_range = [0, 2.8]  # Example range, adjust this as needed\n\n\n# Create the plot\nAHMchart = alt.Chart(AHM_group).mark_line().encode(\n    x='Assessment Year:O', \n    y='B/Bmsy:Q',\n    color='Common Name:N',\n    detail='Common Name:N'\n).properties(\n    width=400,\n    height=400,\n    title=\"B/Bmsy for Each Species Over the Years 2005-2022\"\n)\n\n# Chart the 2007 and 2018 policies\npolicy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n)\n\n# Combine the line chart and vertical lines\nAHM_policychart = AHMchart + policy_lines\n\n# Display the final chart\nAHM_policychart\n\n\n\n\n\n\n\n\nNortheast Shelf Data: *** STILL NEED TO COMPLETE\n\n\nLandings // Fish Haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n\n\nRegional GDP\n\n# Import annual GDP data for each state of interest. \n\n\n# Filter data for the years 2005-2022. \n\n\n# Find mean GDP across all states in our study region for each year from 2005-2022. \n\n\n\n\nEmployment in Fishing Industry\n\n# Import data from Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for all states of interest for 2005-2022\n\n\n# Group data by state and year\n\n\n\n\nCombine all Data in One Graph\n\n# Create a graph combining all three data groups. \n\n# Chart the 2007 and 2018 policies\n# policy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    # x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n# )\n\n# Combine the line chart and vertical lines\n# finalchart = finalchart + policy_lines\n\n# Display the final chart\n# finalchart\n\n\n\n\nQ1 PART 2 / State Scale: Are there state-specific patterns?\nDownload states and county geometries\n\nStates_of_interest = ['Delaware', 'Pennsylvania', 'New Jersey', 'New York', 'Connecticut', 'Rhode Island', 'Massachusetts', 'New Hampshire', 'Maine']\n\n# Open downloaded US Census states shapefile from data folder\nallstates = gpd.read_file(\"./data/cb_2021_us_state_20m/cb_2021_us_state_20m.shp\")\nsel_states = allstates[allstates['NAME'].isin(States_of_interest)]\n\n# Download US coastal counties \n\n\nGDP per Capita\nDetermine GDP per capita for each state from 2005 to 2022\n\n# Import state annual GDP for 2005-2022 by county \n\n\n# Import US Census population count for each state from 2005-2022. \n\n\n# Calculate GDP per capita for each state for 2005-2022. \n\n\n# Calculate the median GDP per capita for 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nEmployment in Fishing Industry\nBureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for each state of interest from 2005 2022\n\n# Import Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county\n\n\n# Group data by yaer and county\n\n\n# Calculate median employment in fishing industry in each state from 2005-2007, 2008-2018, and 2019-2022.\n\n\n\nLandings // Fish haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n# Take median of landings in each state from 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nSpatial Join\n\n# Join GDP per capita, BLS employment, and landings data to the coastal counties gpd. \n\n\n\n\n\nInteractive Map\n\n# Create interactive map of coastal counties from Delaware to Maine region which toggles the three data groups",
    "crumbs": [
      "Analysis",
      "Major Fisheries along the North Atlantic Coast"
    ]
  },
  {
    "objectID": "analysis/2-MajorFisheriesNE.html#question-2",
    "href": "analysis/2-MajorFisheriesNE.html#question-2",
    "title": "Major Fisheries along the North Atlantic Coast",
    "section": "QUESTION 2:",
    "text": "QUESTION 2:\n\nWhere are the most important fisheries along the north Atlantic Coast and what factors define the most important fishing ports?\nSelect based on: - 2022 data - % of regional fish quantity - % GDP from maritime activities of county - % of employment in fishing - Density of fishing vessel activity\n\n\nDownload required libraries\n\nfrom urllib.request import urlopen\nimport json\nimport datetime\nimport requests\n\n\n\nSpecify Region of Interest\nFor this project, we are interested in the fisheres along the north Atlantic Coast from Delaware to Maine.\n\n# Create list of States of Interest\n\nStates_short = ['DE', 'PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'ME']\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n\n\nLandings in 2022 (quantity of fish)\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": \"2022\"}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\nlen(landings_df)\n\n10000\n\n\n\n\nFishing Vessel Activity\n\n# Request vessel data from NOAA Fisheries API \n###(NOTE TO MARIYA: I'm not sure whether it is pulling all of the pages of data. I tried to write a code which cycled through the data pages but couldn't get it to work.)\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/vessel_data/?offset=0&limit=50000\"\nparamsvessel = {\"hail_state\": States_short}\nresponse = requests.get(url, params=paramsvessel)\nvesseldata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = vesseldata['items']\nvessels_df = pd.DataFrame(items)\nlen(vessels_df)\n\n10000",
    "crumbs": [
      "Analysis",
      "Major Fisheries along the North Atlantic Coast"
    ]
  },
  {
    "objectID": "analysis/1-Fisheries_NE-Copy1.html",
    "href": "analysis/1-Fisheries_NE-Copy1.html",
    "title": "FINAL PROJECT",
    "section": "",
    "text": "---\nformat: \n  html:\n    toc: false\n    page-layout: full\nexecute:\n    echo: false\n---\nMariya Lupandina and Clarasophia Gust MUSA 550 | Fall 2024\n# Import necessary libraries \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport zipfile"
  },
  {
    "objectID": "analysis/1-Fisheries_NE-Copy1.html#question-1",
    "href": "analysis/1-Fisheries_NE-Copy1.html#question-1",
    "title": "FINAL PROJECT",
    "section": "QUESTION 1:",
    "text": "QUESTION 1:\nHow was fish stock, GDP, and commercial fishing employment impacted by the two policy changes?\n\nQ1 PART 1 / Regional Scale: Is there a regional pattern?\n\nFish Stock:\nData on fish stock counts from 2005 to 2022 was downloaded from NOAA Stock SMART (https://apps-st.fisheries.noaa.gov/stocksmart?app=download-data). Data on all stocks was downloaded for the two relevant marine ecosystem areas: Atlantic Highly Migratory (AHM) and Northeast Shelf (NES).\n\n# Import fish count data for 2005-2022 from NOAA StockSMART. \n# Data is downloaded by ecosystem area. Load xlsx data for Atlantic Highly Migratory (AHM) and Northeast Shelf (NES) ecosystems. \n    \nAHM_stocks = pd.read_csv(\"./data/AHM_NOAAstockSMART_2005to2022.csv\")\nNES_stocks = pd.read_csv(\"./data/NES_NOAAstockSMART_2005to2022.csv\")\n   \n# Drop rows without values for the ratio of current estimated total biomass and the estimate of total sustainable level of biomass (B/Bmsy).\nAHMstocks_clean = AHM_stocks.dropna(subset=['B/Bmsy'])\nNESstocks_clean = NES_stocks.dropna(subset=['B/Bmsy'])\n\n# Group each ecosystem's data by stock assessment year and species. \nAHM_grouped = AHMstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nNES_grouped = NESstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nstocks_df = combined_df = pd.concat([AHM_grouped, NES_grouped], ignore_index=True)\n\nNES_grouped.columns\n\nAtlantic Highly Migratory data:\n\nAHM_group = AHMstocks_clean.groupby(['Common Name', 'Assessment Year']).median('B/Bmsy').reset_index()\n\n# Ensure the 'Assessment Year' is treated as an integer\nAHMstocks_clean.loc[:, 'Assessment Year'] = AHMstocks_clean['Assessment Year'].astype(int)\n\nB_Bmsy_range = [0, 2.8]  # Example range, adjust this as needed\n\n\n# Create the plot\nAHMchart = alt.Chart(AHM_group).mark_line().encode(\n    x='Assessment Year:O', \n    y='B/Bmsy:Q',\n    color='Common Name:N',\n    detail='Common Name:N'\n).properties(\n    width=400,\n    height=400,\n    title=\"B/Bmsy for Each Species Over the Years 2005-2022\"\n)\n\n# Chart the 2007 and 2018 policies\npolicy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n)\n\n# Combine the line chart and vertical lines\nAHM_policychart = AHMchart + policy_lines\n\n# Display the final chart\nAHM_policychart\n\n\n\n\n\n\n\n\nNortheast Shelf Data: *** STILL NEED TO COMPLETE\n\n\nLandings // Fish Haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n\n\nRegional GDP\n\n# Import annual GDP data for each state of interest. \n\n\n# Filter data for the years 2005-2022. \n\n\n# Find mean GDP across all states in our study region for each year from 2005-2022. \n\n\n\n\nEmployment in Fishing Industry\n\n# Import data from Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for all states of interest for 2005-2022\n\n\n# Group data by state and year\n\n\n\n\nCombine all Data in One Graph\n\n# Create a graph combining all three data groups. \n\n# Chart the 2007 and 2018 policies\n# policy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    # x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n# )\n\n# Combine the line chart and vertical lines\n# finalchart = finalchart + policy_lines\n\n# Display the final chart\n# finalchart\n\n\n\n\nQ1 PART 2 / State Scale: Are there state-specific patterns?\nDownload states and county geometries\n\nStates_of_interest = ['Delaware', 'Pennsylvania', 'New Jersey', 'New York', 'Connecticut', 'Rhode Island', 'Massachusetts', 'New Hampshire', 'Maine']\n\n# Open downloaded US Census states shapefile from data folder\nallstates = gpd.read_file(\"./data/cb_2021_us_state_20m/cb_2021_us_state_20m.shp\")\nsel_states = allstates[allstates['NAME'].isin(States_of_interest)]\n\n# Download US coastal counties \n\n\nGDP per Capita\nDetermine GDP per capita for each state from 2005 to 2022\n\n# Import state annual GDP for 2005-2022 by county \n\n\n# Import US Census population count for each state from 2005-2022. \n\n\n# Calculate GDP per capita for each state for 2005-2022. \n\n\n# Calculate the median GDP per capita for 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nEmployment in Fishing Industry\nBureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for each state of interest from 2005 2022\n\n# Import Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county\n\n\n# Group data by yaer and county\n\n\n# Calculate median employment in fishing industry in each state from 2005-2007, 2008-2018, and 2019-2022.\n\n\n\nLandings // Fish haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n# Take median of landings in each state from 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nSpatial Join\n\n# Join GDP per capita, BLS employment, and landings data to the coastal counties gpd. \n\n\n\n\n\nInteractive Map\n\n# Create interactive map of coastal counties from Delaware to Maine region which toggles the three data groups"
  },
  {
    "objectID": "analysis/1-Fisheries_NE-Copy1.html#question-2",
    "href": "analysis/1-Fisheries_NE-Copy1.html#question-2",
    "title": "FINAL PROJECT",
    "section": "QUESTION 2:",
    "text": "QUESTION 2:\n\nWhere are the most important fisheries along the north Atlantic Coast and what factors define the most important fishing ports?\nSelect based on: - 2022 data - % of regional fish quantity - % GDP from maritime activities of county - % of employment in fishing - Density of fishing vessel activity\n\n\nDownload required libraries\n\nfrom urllib.request import urlopen\nimport json\nimport datetime\nimport requests\n\n\n\nSpecify Region of Interest\nFor this project, we are interested in the fisheres along the north Atlantic Coast from Delaware to Maine.\n\n# Create list of States of Interest\n\nStates_short = ['DE', 'PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'ME']\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n\n\nLandings in 2022 (quantity of fish)\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": \"2022\"}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\nlen(landings_df)\n\n10000\n\n\n\n\nFishing Vessel Activity\n\n# Request vessel data from NOAA Fisheries API \n###(NOTE TO MARIYA: I'm not sure whether it is pulling all of the pages of data. I tried to write a code which cycled through the data pages but couldn't get it to work.)\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/vessel_data/?offset=0&limit=50000\"\nparamsvessel = {\"hail_state\": States_short}\nresponse = requests.get(url, params=paramsvessel)\nvesseldata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = vesseldata['items']\nvessels_df = pd.DataFrame(items)\nlen(vessels_df)\n\n10000"
  },
  {
    "objectID": "analysis/1-FishstockLivelihood.html",
    "href": "analysis/1-FishstockLivelihood.html",
    "title": "Fisheries & Livelihood following Amendments to the MSA",
    "section": "",
    "text": "How was fish stock, GDP, and commercial fishing employment impacted by the two policy changes?\n\nQ1 PART 1 / Regional Scale: Is there a regional pattern?\n\nFish Stock:\nData on fish stock from 2005 to 2022 was downloaded from NOAA Stock SMART (https://apps-st.fisheries.noaa.gov/stocksmart?app=download-data). Data on all stocks was downloaded for the two marine ecosystems which are relevant to the region of study: Atlantic Highly Migratory (AHM) and Northeast Shelf (NES).\nFish Stock of the Atlantic Highly Migratory Marine Ecosystem:\n\n\n\n\n\n\n\n\nFish Stock of the Northeast Shelf Marine Ecosystem:\n\n\n\n\n\n\n\n\n\n\nLandings // Fish Haul\n\n\n\n\n\n\n\n\n\n\nRegional GDP\n\n# Import annual GDP data for each state of interest. \n\n\n# Filter data for the years 2005-2022. \n\n\n# Find mean GDP across all states in our study region for each year from 2005-2022. \n\n\n\n\nEmployment in Fishing Industry\n\n# Import data from Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for all states of interest for 2005-2022\n\n\n# Group data by state and year\n\n\n\n\nCombine all Data in One Graph\n\n# Create a graph combining all three data groups. \n\n# Chart the 2007 and 2018 policies\n# policy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    # x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n# )\n\n# Combine the line chart and vertical lines\n# finalchart = finalchart + policy_lines\n\n# Display the final chart\n# finalchart\n\n\n\n\nQ1 PART 2 / State Scale: Are there state-specific patterns?\nDownload states and county geometries\n\nStates_of_interest = ['Delaware', 'Pennsylvania', 'New Jersey', 'New York', 'Connecticut', 'Rhode Island', 'Massachusetts', 'New Hampshire', 'Maine']\n\n# Open downloaded US Census states shapefile from data folder\nallstates = gpd.read_file(\"./data/cb_2021_us_state_20m/cb_2021_us_state_20m.shp\")\nsel_states = allstates[allstates['NAME'].isin(States_of_interest)]\n\n# Download US coastal counties \n\n\nGDP per Capita\nDetermine GDP per capita for each state from 2005 to 2022\n\n# Import state annual GDP for 2005-2022 by county \n\n\n# Import US Census population count for each state from 2005-2022. \n\n\n# Calculate GDP per capita for each state for 2005-2022. \n\n\n# Calculate the median GDP per capita for 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nEmployment in Fishing Industry\nBureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for each state of interest from 2005 2022\n\n# Import Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county\n\n\n# Group data by yaer and county\n\n\n# Calculate median employment in fishing industry in each state from 2005-2007, 2008-2018, and 2019-2022.\n\n\n\nLandings // Fish haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n# Take median of landings in each state from 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nSpatial Join\n\n# Join GDP per capita, BLS employment, and landings data to the coastal counties gpd. \n\n\n\n\n\nInteractive Map\n\n# Create interactive map of coastal counties from Delaware to Maine region which toggles the three data groups",
    "crumbs": [
      "Analysis",
      "Fisheries & Livelihood following Amendments to the MSA"
    ]
  },
  {
    "objectID": "analysis/1-FishstockLivelihood.html#question-1",
    "href": "analysis/1-FishstockLivelihood.html#question-1",
    "title": "Fisheries & Livelihood following Amendments to the MSA",
    "section": "QUESTION 1:",
    "text": "QUESTION 1:\nHow was fish stock, GDP, and commercial fishing employment impacted by the two policy changes?\n\nQ1 PART 1 / Regional Scale: Is there a regional pattern?\n\nFish Stock:\nData on fish stock counts from 2005 to 2022 was downloaded from NOAA Stock SMART (https://apps-st.fisheries.noaa.gov/stocksmart?app=download-data). Data on all stocks was downloaded for the two relevant marine ecosystem areas: Atlantic Highly Migratory (AHM) and Northeast Shelf (NES).\n\n# Import fish count data for 2005-2022 from NOAA StockSMART. \n# Data is downloaded by ecosystem area. Load xlsx data for Atlantic Highly Migratory (AHM) and Northeast Shelf (NES) ecosystems. \n    \nAHM_stocks = pd.read_csv(\"./data/AHM_NOAAstockSMART_2005to2022.csv\")\nNES_stocks = pd.read_csv(\"./data/NES_NOAAstockSMART_2005to2022.csv\")\n   \n# Drop rows without values for the ratio of current estimated total biomass and the estimate of total sustainable level of biomass (B/Bmsy).\nAHMstocks_clean = AHM_stocks.dropna(subset=['B/Bmsy'])\nNESstocks_clean = NES_stocks.dropna(subset=['B/Bmsy'])\n\n# Group each ecosystem's data by stock assessment year and species. \nAHM_grouped = AHMstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nNES_grouped = NESstocks_clean.groupby(['Stock Name', 'Stock ID', 'Assessment Year']).agg({\n    'Estimated B': 'median',\n    'B/Bmsy': 'median'\n})\n\nstocks_df = combined_df = pd.concat([AHM_grouped, NES_grouped], ignore_index=True)\n\nNES_grouped.columns\n\nAtlantic Highly Migratory data:\n\nAHM_group = AHMstocks_clean.groupby(['Common Name', 'Assessment Year']).median('B/Bmsy').reset_index()\n\n# Ensure the 'Assessment Year' is treated as an integer\nAHMstocks_clean.loc[:, 'Assessment Year'] = AHMstocks_clean['Assessment Year'].astype(int)\n\nB_Bmsy_range = [0, 2.8]  # Example range, adjust this as needed\n\n\n# Create the plot\nAHMchart = alt.Chart(AHM_group).mark_line().encode(\n    x='Assessment Year:O', \n    y='B/Bmsy:Q',\n    color='Common Name:N',\n    detail='Common Name:N'\n).properties(\n    width=400,\n    height=400,\n    title=\"B/Bmsy for Each Species Over the Years 2005-2022\"\n)\n\n# Chart the 2007 and 2018 policies\npolicy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n)\n\n# Combine the line chart and vertical lines\nAHM_policychart = AHMchart + policy_lines\n\n# Display the final chart\nAHM_policychart\n\n\n\n\n\n\n\n\nNortheast Shelf Data: *** STILL NEED TO COMPLETE\n\n\nLandings // Fish Haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n\n\nRegional GDP\n\n# Import annual GDP data for each state of interest. \n\n\n# Filter data for the years 2005-2022. \n\n\n# Find mean GDP across all states in our study region for each year from 2005-2022. \n\n\n\n\nEmployment in Fishing Industry\n\n# Import data from Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for all states of interest for 2005-2022\n\n\n# Group data by state and year\n\n\n\n\nCombine all Data in One Graph\n\n# Create a graph combining all three data groups. \n\n# Chart the 2007 and 2018 policies\n# policy_lines = alt.Chart(pd.DataFrame({'year': [2007, 2018]})).mark_rule(color='black').encode(\n    # x='year:O'  # 'O' is for ordinal scale, as we are using specific years as markers\n# )\n\n# Combine the line chart and vertical lines\n# finalchart = finalchart + policy_lines\n\n# Display the final chart\n# finalchart\n\n\n\n\nQ1 PART 2 / State Scale: Are there state-specific patterns?\nDownload states and county geometries\n\nStates_of_interest = ['Delaware', 'Pennsylvania', 'New Jersey', 'New York', 'Connecticut', 'Rhode Island', 'Massachusetts', 'New Hampshire', 'Maine']\n\n# Open downloaded US Census states shapefile from data folder\nallstates = gpd.read_file(\"./data/cb_2021_us_state_20m/cb_2021_us_state_20m.shp\")\nsel_states = allstates[allstates['NAME'].isin(States_of_interest)]\n\n# Download US coastal counties \n\n\nGDP per Capita\nDetermine GDP per capita for each state from 2005 to 2022\n\n# Import state annual GDP for 2005-2022 by county \n\n\n# Import US Census population count for each state from 2005-2022. \n\n\n# Calculate GDP per capita for each state for 2005-2022. \n\n\n# Calculate the median GDP per capita for 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nEmployment in Fishing Industry\nBureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county for each state of interest from 2005 2022\n\n# Import Bureau of Labour Statistics (BLS) employment data for NAICS 1141 - Fishing by county\n\n\n# Group data by yaer and county\n\n\n# Calculate median employment in fishing industry in each state from 2005-2007, 2008-2018, and 2019-2022.\n\n\n\nLandings // Fish haul\n\n# Specify years of interest and states\nyears = [range(2005,2023)]\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": years}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested landings data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\n\n# Group data by state and year\n\n\n# Take median of landings in each state from 2005-2007, 2008-2018, and 2019-2022. \n\n\n\nSpatial Join\n\n# Join GDP per capita, BLS employment, and landings data to the coastal counties gpd. \n\n\n\n\n\nInteractive Map\n\n# Create interactive map of coastal counties from Delaware to Maine region which toggles the three data groups",
    "crumbs": [
      "Analysis",
      "Fisheries & Livelihood following Amendments to the MSA"
    ]
  },
  {
    "objectID": "analysis/1-FishstockLivelihood.html#question-2",
    "href": "analysis/1-FishstockLivelihood.html#question-2",
    "title": "Fisheries & Livelihood following Amendments to the MSA",
    "section": "QUESTION 2:",
    "text": "QUESTION 2:\n\nWhere are the most important fisheries along the north Atlantic Coast and what factors define the most important fishing ports?\nSelect based on: - 2022 data - % of regional fish quantity - % GDP from maritime activities of county - % of employment in fishing - Density of fishing vessel activity\n\n\nDownload required libraries\n\nfrom urllib.request import urlopen\nimport json\nimport datetime\nimport requests\n\n\n\nSpecify Region of Interest\nFor this project, we are interested in the fisheres along the north Atlantic Coast from Delaware to Maine.\n\n# Create list of States of Interest\n\nStates_short = ['DE', 'PA', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'ME']\nStates_long = ['DELAWARE', 'PENNSYLVANIA', 'NEW JERSEY', 'NEW YORK', 'CONNECTICUT', 'RHODE ISLAND', 'MASSACHUSETTS', 'NEW HAMPSHIRE', 'MAINE']\n\n\n\nLandings in 2022 (quantity of fish)\n\n# Reqest landings data from NOAA Fisheries API\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/landings/?offset=0&limit=100000\"\nparams = {\"state_name\": States_long, \"year\": \"2022\"}\nresponse = requests.get(url, params=params)\nlandingsdata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = landingsdata['items']\nlandings_df = pd.DataFrame(items)\nlen(landings_df)\n\n10000\n\n\n\n\nFishing Vessel Activity\n\n# Request vessel data from NOAA Fisheries API \n###(NOTE TO MARIYA: I'm not sure whether it is pulling all of the pages of data. I tried to write a code which cycled through the data pages but couldn't get it to work.)\n\nurl = \"https://apps-st.fisheries.noaa.gov/ods/foss/vessel_data/?offset=0&limit=50000\"\nparamsvessel = {\"hail_state\": States_short}\nresponse = requests.get(url, params=paramsvessel)\nvesseldata = response.json()\n\n# Turn the requested vessel data into a data frame\n\nitems = vesseldata['items']\nvessels_df = pd.DataFrame(items)\nlen(vessels_df)\n\n10000",
    "crumbs": [
      "Analysis",
      "Fisheries & Livelihood following Amendments to the MSA"
    ]
  }
]